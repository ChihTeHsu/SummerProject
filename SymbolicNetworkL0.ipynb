{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SymbolicNetworkL0.ipynb","provenance":[],"authorship_tag":"ABX9TyOlKTjePUZcbfFUPwHcHLbx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"id":"gcpgSUiR-km8","executionInfo":{"status":"ok","timestamp":1607441674390,"user_tz":360,"elapsed":14549,"user":{"displayName":"Chihte Hsu","photoUrl":"","userId":"17267872587667426076"}},"outputId":"28b7bc1d-5612-4fc8-e512-9bdf3e9c15ea"},"source":["!pip install sympy==1.5\n","import sympy\n","sympy.__version__"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting sympy==1.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/a7/25d5d6b3295537ab90bdbcd21e464633fb4a0684dd9a065da404487625bb/sympy-1.5-py2.py3-none-any.whl (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 5.9MB/s \n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy==1.5) (1.1.0)\n","Installing collected packages: sympy\n","  Found existing installation: sympy 1.1.1\n","    Uninstalling sympy-1.1.1:\n","      Successfully uninstalled sympy-1.1.1\n","Successfully installed sympy-1.5\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["sympy"]}}},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.1.1'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HCIMylzi-tXQ","executionInfo":{"status":"ok","timestamp":1607441694904,"user_tz":360,"elapsed":1453,"user":{"displayName":"Chihte Hsu","photoUrl":"","userId":"17267872587667426076"}},"outputId":"f4259ae6-78cd-4126-d7c4-f2d6eed40450"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/My Drive/Library') #!ls /content/gdrive/My\\ Drive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0eShsZYq_LXf","executionInfo":{"status":"ok","timestamp":1607441699784,"user_tz":360,"elapsed":2467,"user":{"displayName":"Chihte Hsu","photoUrl":"","userId":"17267872587667426076"}}},"source":["import tensorflow as tf\n","import numpy as np\n","from utils import functions\n","\n","# Constants for L0 Regularization\n","BETA = 2 / 3\n","GAMMA = -0.1\n","ZETA = 1.1\n","EPSILON = 1e-6\n","\n","class SymbolicLayerL0(tf.keras.layers.Layer):\n","    def __init__(self, funcs=None, initial_weight=None, variable=False, init_stddev=0.1):\n","        super().__init__()\n","        \n","        if funcs is None:\n","            funcs = functions.default_func\n","        self.initial_weight = initial_weight\n","        self.W = None       # Weight matrix\n","        self.built = False  # Boolean whether weights have been initialized\n","        if self.initial_weight is not None:     # use the given initial weight\n","            with tf.name_scope(\"symbolic_layer\"):\n","                if not variable:\n","                    self.W = tf.Variable(self.initial_weight)\n","                else:\n","                    self.W = self.initial_weight\n","            self.built = True\n","        \n","        self.init_stddev = init_stddev\n","        self.n_funcs = len(funcs)           # Number of activation functions (and number of layer outputs)\n","        self.funcs = [func.tf for func in funcs]        # Convert functions to list of Tensorflow functions\n","        self.n_double = functions.count_double(funcs)   # Number of activation functions that take 2 inputs\n","        self.n_single = self.n_funcs - self.n_double    # Number of activation functions that take 1 input\n","        \n","        self.out_dim = self.n_funcs + self.n_double\n","        self.droprate_init = 0.5\n","        self.bias = None\n","        self.qz_log_alpha = None\n","        self.in_dim = None\n","        self.eps = None\n","    \n","    def build(self, in_dim):\n","        with tf.name_scope(\"symbolic_layer\"):\n","            self.in_dim = in_dim\n","            #self.qz_log_alpha = tf.Variable(tf.zeros(shape=(self.in_dim, self.out_dim)))\n","            self.qz_log_alpha = tf.Variable(tf.random.normal((in_dim, self.out_dim),\n","                                                             mean=tf.math.log(1-self.droprate_init) - tf.math.log(self.droprate_init),\n","                                                             stddev=1e-2))\n","    \n","    def loss(self):\n","        \"\"\"Regularization loss term\"\"\"\n","        return tf.reduce_sum(tf.sigmoid(self.qz_log_alpha - BETA * tf.math.log(-GAMMA / ZETA)))\n","    \n","    def call(self, x, sample=True, reuse_u=False):\n","        \"\"\"Multiply by weight matrix and apply activation units\"\"\"\n","        with tf.name_scope(\"symbolic_layer\"):\n","            if self.W is None or self.qz_log_alpha is None:\n","                self.build(x.shape.as_list()[1])    # First dimension is batch size\n","            \n","            if sample:\n","                \"\"\"Uniform random numbers for concrete distribution\"\"\"\n","                if self.eps is None or not reuse_u:\n","                    #self.eps = tf.zeros(shape=(self.in_dim, self.out_dim))\n","                    self.eps = tf.random.uniform(shape=(self.in_dim, self.out_dim), minval=EPSILON, maxval=1.0 - EPSILON)\n","                \"\"\"Quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n","                y = tf.sigmoid((tf.math.log(self.eps) - tf.math.log(1.0-self.eps) + self.qz_log_alpha) / BETA)\n","                z = y * (ZETA - GAMMA) + GAMMA\n","                mask = tf.clip_by_value(z, clip_value_min=0.0, clip_value_max=1.0)\n","                w = self.W * mask\n","                h = tf.matmul(x, w)\n","            else:\n","                \"\"\"Deterministic value of weight based on mean of z\"\"\"\n","                pi = tf.sigmoid(self.qz_log_alpha)\n","                z_mean = tf.clip_by_value(pi * (ZETA - GAMMA) + GAMMA, clip_value_min=0.0, clip_value_max=1.0)\n","                w = self.W * z_mean\n","                h = tf.matmul(x, w)\n","            \n","            output = []\n","            in_i = 0    # input index\n","            out_i = 0   # output index\n","            # Apply functions with only a single input\n","            while out_i < self.n_single:\n","                output.append(self.funcs[out_i](h[:, in_i]))\n","                in_i += 1\n","                out_i += 1\n","            # Apply functions that take 2 inputs and produce 1 output\n","            while out_i < self.n_funcs:\n","                output.append(self.funcs[out_i](h[:, in_i], h[:, in_i+1]))\n","                in_i += 2\n","                out_i += 1\n","            output = tf.stack(output, axis=1)\n","            return output\n","\n","class SymbolicNetL0(tf.keras.Model):\n","    def __init__(self, symbolic_depth, funcs=None, initial_weights=None, initial_bias=None,\n","                 variable=False, init_stddev=0.1):\n","        super().__init__()     # Python 2 下使用 super(MyModel, self).__init__()\n","        # 此處添加初始化程式碼（包含 call 方法中會用到的層），例如\n","        self.depth = symbolic_depth     # Number of hidden layers\n","        self.funcs = funcs\n","        self.shape = (None, 1)\n","        if initial_weights is not None:\n","            self.symbolic_layers = [SymbolicLayerL0(funcs=funcs, initial_weight=initial_weights[i], variable=variable)\n","                                    for i in range(self.depth)]\n","            if not variable:\n","                self.output_weight = tf.Variable(initial_weights[-1])\n","            else:\n","                self.output_weight = initial_weights[-1]\n","    \n","    def call(self, input, sample=True, reuse_u=False):\n","        # 此處添加模型呼叫的程式碼（處理輸入並返回輸出），例如\n","        self.shape = (int(input.shape[1]), 1)     # Dimensionality of the input\n","        # connect output from previous layer to input of next layer\n","        h = input\n","        # Building hidden layers\n","        for i in range(self.depth):\n","            h = self.symbolic_layers[i](h, sample=sample, reuse_u=reuse_u)\n","        # Final output (no activation units) of network\n","        h = tf.matmul(h, self.output_weight)\n","        return h\n","    \n","    def get_loss(self):\n","        return tf.reduce_sum([self.symbolic_layers[i].loss() for i in range(self.depth)])\n","    \n","    def get_weights(self):\n","        \"\"\"Return list of weight matrices, TF.Keras method w/ slightly tweak output Type\"\"\"\n","        # First part is iterating over hidden weights. Then append the output weight.        \n","        return [self.symbolic_layers[i].W * tf.clip_by_value(\n","                tf.sigmoid(self.symbolic_layers[i].qz_log_alpha) * (ZETA - GAMMA) + GAMMA,clip_value_min=0.0,clip_value_max=1.0)\n","                for i in range(self.depth)] + [self.output_weight]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdk-fPXS_hnN","executionInfo":{"status":"ok","timestamp":1607441788994,"user_tz":360,"elapsed":13235,"user":{"displayName":"Chihte Hsu","photoUrl":"","userId":"17267872587667426076"}},"outputId":"256732bf-c0b1-4705-8fcc-89a7e8257698"},"source":["from inspect import signature\n","func=lambda x, y: x**2 + np.sin(2*np.pi*y)\n","x_dim = len(signature(func).parameters)     # Number of inputs to the function, or, dimensionality of x\n","N_TRAIN=256\n","NOISE_SD = 0\n","range_min=-1\n","range_max=1\n","x = np.array((range_max - range_min) * np.random.random([N_TRAIN, x_dim]) + range_min, dtype=np.float32)\n","y = np.array(np.random.normal([[func(*x_i)] for x_i in x], NOISE_SD), dtype=np.float32)\n","\n","activation_funcs = [\n","            *[functions.Constant()] * 2,\n","            *[functions.Identity()] * 4,\n","            *[functions.Square()] * 4,\n","            *[functions.Sin()] * 2,\n","            *[functions.Exp()] * 2,\n","            *[functions.Sigmoid()] * 2,\n","            *[functions.Product()] * 2\n","        ]\n","width = len(activation_funcs)\n","n_double = functions.count_double(activation_funcs) #count how many 2-in func\n","\n","summary_step = 1000 # Number of iterations at which to print to screen\n","n_epochs1=10001\n","n_epochs2=10001\n","\n","init_sd_first = 0.5\n","init_sd_last = 0.5\n","init_sd_middle = 0.5\n","initial_weights=[tf.random.truncated_normal([x_dim, width + n_double], stddev=init_sd_first),\n","                 tf.random.truncated_normal([width, width + n_double], stddev=init_sd_middle),\n","                 tf.random.truncated_normal([width, 1], stddev=init_sd_last)]\n","\n","sym = SymbolicNetL0(len(initial_weights)-1,\n","                    funcs=activation_funcs,\n","                    initial_weights=initial_weights)\n","optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-2)\n","\n","@tf.function\n","def train_one_step(x,y):\n","    with tf.GradientTape() as tape:\n","        y_hat = sym(x)\n","        error = tf.math.reduce_mean(tf.keras.losses.mean_squared_error(y, y_hat))\n","        reg_loss = sym.get_loss()\n","        loss = error + 0.005 * reg_loss\n","    grads = tape.gradient(loss, sym.variables)    # 使用 model.variables 這一屬性直接獲得模型中的所有變數\n","    optimizer.apply_gradients(grads_and_vars=zip(grads, sym.variables))\n","    return loss\n","\n","for i in range(n_epochs1):\n","    loss_val=train_one_step(x,y)\n","    if i % 1000 == 0:   # Number of iterations at which to print to screen\n","        print(\"Epoch: %d - loss: %f\" % (i,loss_val))\n","\n","from utils import pretty_print\n","print(pretty_print.network([tf.convert_to_tensor(w) for w in sym.get_weights()],\n","                           activation_funcs,\n","                           [\"x\",\"y\"]))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Epoch: 0 - loss: 3.841239\n","Epoch: 1000 - loss: 0.363843\n","Epoch: 2000 - loss: 0.697265\n","Epoch: 3000 - loss: 0.062612\n","Epoch: 4000 - loss: 0.041749\n","Epoch: 5000 - loss: 0.041766\n","Epoch: 6000 - loss: 0.033600\n","Epoch: 7000 - loss: 0.027199\n","Epoch: 8000 - loss: 0.020205\n","Epoch: 9000 - loss: 0.028498\n","Epoch: 10000 - loss: 0.021169\n","1.04959*x**2 + 0.997291*sin(6.28749213800495*y) + 0.0311579\n"],"name":"stdout"}]}]}