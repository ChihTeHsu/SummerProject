{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/My Drive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3gscg9QZ4A",
        "outputId": "2214ac06-3c82-4885-b88a-0c583a6f8811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = pd.read_csv('accepted_2007_to_2018Q4.csv',parse_dates=['issue_d'], infer_datetime_format=True)\n",
        "data.drop(data.tail(2).index, inplace=True)"
      ],
      "metadata": {
        "id": "LExr1nKrAM7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3fd51d-9f46-47f2-8402-98d8269edd09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_feat = ['acc_now_delinq','acc_open_past_24mths','all_util','annual_inc','avg_cur_bal','bc_open_to_buy','bc_util',\n",
        "            'chargeoff_within_12_mths','collections_12_mths_ex_med','delinq_2yrs','delinq_amnt','dti','earliest_cr_line',\n",
        "            'fico_range_high','fico_range_low','il_util','inq_fi','inq_last_12m','inq_last_6mths','installment','int_rate',\n",
        "            'last_fico_range_high','last_fico_range_low','max_bal_bc','mo_sin_old_il_acct','mo_sin_old_rev_tl_op',\n",
        "            'mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl','mort_acc','mths_since_last_delinq','mths_since_last_major_derog',\n",
        "            'mths_since_last_record','mths_since_rcnt_il','mths_since_recent_bc','mths_since_recent_bc_dlq',\n",
        "            'mths_since_recent_inq','mths_since_recent_revol_delinq','num_accts_ever_120_pd','num_actv_bc_tl',\n",
        "            'num_actv_rev_tl','num_bc_sats','num_bc_tl','num_il_tl','num_op_rev_tl','num_rev_accts','num_rev_tl_bal_gt_0',\n",
        "            'num_sats','num_tl_120dpd_2m','num_tl_30dpd','num_tl_90g_dpd_24m','num_tl_op_past_12m','open_acc','open_acc_6m',\n",
        "            'open_il_12m','open_il_24m','open_rv_12m','open_rv_24m','pct_tl_nvr_dlq','percent_bc_gt_75','pub_rec',\n",
        "            'pub_rec_bankruptcies','revol_util','tot_coll_amt','tot_cur_bal','tot_hi_cred_lim','total_acc','total_bal_ex_mort',\n",
        "            'total_bal_il','total_bc_limit','total_cu_tl','total_il_high_credit_limit','total_rev_hi_lim']\n",
        "\n",
        "cat_feat = ['term','verification_status','sub_grade']\n",
        "\n",
        "X = data[ ['issue_d'] + cat_feat + num_feat].copy()\n",
        "#X.info()\n",
        "\n",
        "# The features earliest_cr_line is date and its type should be changed to datetime. Later \n",
        "# it need to be transformed to ordinal numeric features\n",
        "X['earliest_cr_line'] = pd.to_numeric(pd.to_datetime(X['earliest_cr_line'], infer_datetime_format=True))\n",
        "\n",
        "#print(data['loan_status'].value_counts())\n",
        "y = data['loan_status'].copy()\n",
        "y = (~y.isin(['Current', 'Fully Paid', 'In Grace Period'])).astype('int')\n",
        "y.value_counts()\n",
        "\n",
        "del data"
      ],
      "metadata": {
        "id": "KVDiI2xqRuKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_feat = X.select_dtypes('object').columns.values\n",
        "print(X[cat_feat].nunique().sort_values())\n",
        "\n",
        "X['term'].replace([' 36 months',' 60 months'], [int(0), int(1)], inplace=True)\n",
        "\n",
        "X['verification_status'].replace(['Not Verified','Source Verified', 'Verified'], [int(0), int(1), int(2)], inplace=True)\n",
        "\n",
        "X['sub_grade'].replace(['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5','C1','C2','C3','C4','C5',\n",
        "                        'D1','D2','D3','D4','D5','E1','E2','E3','E4','E5','F1','F2','F3','F4','F5',\n",
        "                        'G1','G2','G3','G4','G5'], [int(x) for x in range(35)], inplace=True)\n",
        "\n",
        "#%%\n",
        "print(X.shape)\n",
        "\n",
        "idx = np.where(\n",
        "    (X['annual_inc'] <= 250000) & (X['dti'] <= 50) & (X['open_acc'] <= 40) &\n",
        "    (X['total_acc'] <= 80) & (X['revol_util'] <= 120) & (X['earliest_cr_line'] > -2E18)\n",
        "    )[0]\n",
        "X = X.iloc[idx]\n",
        "y = y.iloc[idx]\n",
        "print(X.shape)\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, random_state=0)\n",
        "\n",
        "idx = np.where(X['issue_d'].dt.year < 2017.)[0]\n",
        "X_train = X.iloc[idx].drop('issue_d', axis='columns')\n",
        "y_train = y.iloc[idx]\n",
        "\n",
        "idx = np.where( (X['issue_d'].dt.year == 2017.)&(X['issue_d'].dt.month < 8.) )[0]\n",
        "X_val = X.iloc[idx].drop('issue_d', axis='columns')\n",
        "y_val = y.iloc[idx]\n",
        "\n",
        "idx = np.where( ((X['issue_d'].dt.year == 2017.)&(X['issue_d'].dt.month >= 8.))|(X['issue_d'].dt.year >= 2018.) )[0]\n",
        "X_test = X.iloc[idx].drop('issue_d', axis='columns')\n",
        "y_test = y.iloc[idx]\n",
        "\n",
        "print('Training prob Bad',np.mean(y_train))\n",
        "print('Validation prob Bad',np.mean(y_val))\n",
        "print('Test prob Bad',np.mean(y_test))\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbbxMh76Rvw7",
        "outputId": "16aa4796-9131-4dd9-9feb-3eb0a2d6596d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "term                    2\n",
            "verification_status     3\n",
            "sub_grade              35\n",
            "dtype: int64\n",
            "(2260699, 76)\n",
            "(2213429, 76)\n",
            "Training prob Bad 0.1750958388833793\n",
            "Validation prob Bad 0.12304089505846083\n",
            "Test prob Bad 0.05269638093425369\n",
            "(1304794, 75)\n",
            "(236141, 75)\n",
            "(672494, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "tf.keras.backend.clear_session()\n",
        " \n",
        "class BatchNormFCLayer(tf.keras.layers.Layer):  # for the case of fully connected (1D inputs)\n",
        "    \"\"\"\n",
        "    We create class variables such as gamma and beta to be trained. Also creating pop_mean and pop_var\n",
        "    that are expected to grow up to obtain estimates for the population mean and population variance.\n",
        "    The exponential moving average is simple and lets us avoid extra work than moving average.\n",
        "    It is known that the α of the EMA is associated with window length n like this: α =2/(n+1)\n",
        "    \"\"\"\n",
        "    def __init__(self, decay = 0.999, epsilon=0.00000001):\n",
        "        \"\"\"\n",
        "        :param epsilon:\n",
        "        :param decay:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        gamma_init = tf.ones_initializer()\n",
        "        self.gamma = tf.Variable(initial_value=gamma_init(shape=[1], dtype='float32'), trainable=True)\n",
        "       \n",
        "        beta_init = tf.zeros_initializer()\n",
        "        self.beta = tf.Variable(initial_value=beta_init(shape=[1], dtype='float32'), trainable=True)\n",
        "       \n",
        "        self.decay = decay\n",
        "        self.epsilon = epsilon\n",
        "    \"\"\"\n",
        "    In general, we might need to know the shape of the input before runtime in which case we can implement\n",
        "    the build() method in our layer which takes input_shape as a parameter and is called when we compile.\n",
        "    \"\"\"\n",
        "    def build(self, input_shape):  # should pass shape for our variable\n",
        "        self.pop_mean = tf.Variable(tf.zeros([input_shape[-1]]), trainable=False)\n",
        "        self.pop_var = tf.Variable(tf.ones([input_shape[-1]]), trainable=False)\n",
        "        super().build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None):  # Defines the computation from inputs to outputs\n",
        "        \"\"\"\n",
        "        DON'T USE 'training=True' when built by Functional API. If doing that then we force the layer be in\n",
        "        the training mode even during the inference.\n",
        "        Example:\n",
        "        inputs = tf.keras.Input(shape=(10))\n",
        "        x = tf.keras.layers.Dropout(rate=0.5)(inputs, training=True) #<- take training=True out makes inference correct\n",
        "        model = tf.keras.Model(inputs=inputs,outputs=x)\n",
        "        model.predict(tf.constant([1]*10))\n",
        "        \"\"\"\n",
        "        if mask is None:\n",
        "            mask = tf.ones_like(inputs)\n",
        "        prob_mask = tf.math.divide_no_nan(mask, tf.math.reduce_sum( mask, axis=0 ))  # Mask: real value = 1, NaN value = 0\n",
        "\n",
        "        mu = tf.math.reduce_sum( tf.math.multiply_no_nan(inputs, prob_mask) , axis=0 )\n",
        "        variance = tf.math.reduce_sum( tf.math.multiply_no_nan( inputs**2, prob_mask) , axis=0 ) - mu ** 2\n",
        "\n",
        "        if training:  # In case of training, perform batch normalization to learn beta and gamma\n",
        "            self.pop_mean.assign( self.pop_mean * self.decay + mu * (1 - self.decay) )\n",
        "            self.pop_var.assign( self.pop_var * self.decay + variance * (1 - self.decay) )\n",
        "            #outputs = tf.nn.batch_normalization(inputs, mu, variance, self.beta, self.gamma, K.epsilon())\n",
        "            x_hat = (inputs - mu)/tf.sqrt(tf.maximum(variance, K.epsilon()))\n",
        "           \n",
        "        else:  # In case of testing - calculation of the inference model\n",
        "            #outputs = tf.nn.batch_normalization(inputs, self.pop_mean, self.pop_var, self.beta, self.gamma, K.epsilon())\n",
        "            x_hat = (inputs - self.pop_mean)/tf.sqrt(tf.maximum(self.pop_var, K.epsilon()))\n",
        "           \n",
        "        x_hat = tf.math.multiply_no_nan(x_hat, mask) # put non-real value to zeros\n",
        "        outputs = self.gamma * x_hat + self.beta # + (mask-1)*9 # replace non-real value\n",
        "        return outputs\n",
        "\n",
        "class GBN(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Ghost Batch Normalization\n",
        "    https://arxiv.org/abs/1705.08741\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, virtual_batch_size, decay=0.999):\n",
        "        super(GBN, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.num_splits = int(np.ceil(self.batch_size / self.virtual_batch_size))\n",
        "        self.bn = BatchNormFCLayer(decay)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.bn.build(input_shape)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None):\n",
        "        if tf.math.ceil(tf.shape(inputs)[0] / self.virtual_batch_size)< self.num_splits:\n",
        "            if mask is None:\n",
        "                res = self.bn(inputs, None, training)\n",
        "            else:\n",
        "                res = self.bn(inputs, mask, training)\n",
        "            output = res\n",
        "        else:\n",
        "            inp_chunks = tf.split(inputs, self.num_splits, axis=0)\n",
        "            if mask is None:\n",
        "                res = [self.bn(x_, None, training) for x_ in inp_chunks]\n",
        "            else:\n",
        "                mask_chunks = tf.split(mask, self.num_splits, axis=0)\n",
        "                res = [self.bn(x_, m_, training) for x_, m_ in zip(inp_chunks, mask_chunks)]\n",
        "            output = tf.concat(res, axis=0)\n",
        "        \n",
        "        return output\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    Args:\n",
        "        q: query shape == [..., seq_len_q, depth]\n",
        "        k: key shape == [..., seq_len_k, depth]\n",
        "        v: value shape == [..., seq_len_v, depth_v]\n",
        "        mask: Float tensor with shape broadcastable to [..., seq_len_q, seq_len_k].\n",
        "    \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    \n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    \n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    \n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # [..., seq_len_q, seq_len_k]\n",
        "    output = tf.matmul(attention_weights, v)  # [..., seq_len_q, depth_v]\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, output_dim):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        assert d_model % self.num_heads == 0\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "        self.dense = tf.keras.layers.Dense(output_dim)\n",
        "    \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into [num_heads, depth].\n",
        "        Transpose the result such that the shape is [batch_size, num_heads, seq_len, depth]\n",
        "        \"\"\"\n",
        "        # [batch_size, seq_len, num_heads, depth]\n",
        "        x = tf.reshape(x, shape=(batch_size, -1, self.num_heads, self.depth)) \n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]  # q.shape: [batch_size, seq_len, d_model]\n",
        "        \n",
        "        q = self.wq(q)  # [batch_size, seq_len, d_model]\n",
        "        k = self.wk(k)  # [batch_size, seq_len, d_model]\n",
        "        v = self.wv(v)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # [batch_size, num_heads, seq_len_q, depth]\n",
        "        k = self.split_heads(k, batch_size)  # [batch_size, num_heads, seq_len_k, depth]\n",
        "        v = self.split_heads(v, batch_size)  # [batch_size, num_heads, seq_len_v, depth]\n",
        "        \n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        \n",
        "        # scaled_attention.shape == [batch_size, num_heads, seq_len_q, depth]\n",
        "        # attention_weights.shape == [batch_size, num_heads, seq_len_q, seq_len_k]\n",
        "        \n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        # [batch_size, seq_len_q, num_heads, depth]\n",
        "        concat_attention = tf.reshape(scaled_attention, shape=(batch_size, -1, self.d_model)) \n",
        "        # [batch_size, seq_len_q, d_model]\n",
        "        \n",
        "        output = self.dense(concat_attention)  # [batch_size, seq_len_q, output_dim]\n",
        "        return output, attention_weights\n",
        "\n",
        "import enum\n",
        "import math\n",
        "class _TokenInitialization(enum.Enum):\n",
        "    UNIFORM = 'uniform'\n",
        "    NORMAL = 'normal'\n",
        "\n",
        "    @classmethod\n",
        "    def from_str(cls, initialization: str) -> '_TokenInitialization':\n",
        "        try:\n",
        "            return cls(initialization)\n",
        "        except ValueError:\n",
        "            valid_values = [x.value for x in _TokenInitialization]\n",
        "            raise ValueError(f'initialization must be one of {valid_values}')\n",
        "    \n",
        "    def apply(self, n_features: int, d_token: int) -> tf.Variable:\n",
        "        d_sqrt_inv = 1 / math.sqrt(d_token)\n",
        "        if self == _TokenInitialization.UNIFORM:\n",
        "            # used in the paper \"Revisiting Deep Learning Models for Tabular Data\";\n",
        "            # is equivalent to `nn.init.kaiming_uniform_(x, a=math.sqrt(5))` (which is\n",
        "            # used by torch to initialize nn.Linear.weight, for example)\n",
        "            initializer = tf.random_uniform_initializer(minval=-d_sqrt_inv, maxval=d_sqrt_inv)\n",
        "            return tf.Variable(initial_value=initializer(shape=(n_features, d_token), dtype=\"float32\"),trainable=True,)\n",
        "            #nn.init.uniform_(x, a=-d_sqrt_inv, b=d_sqrt_inv)\n",
        "        elif self == _TokenInitialization.NORMAL:\n",
        "            #nn.init.normal_(x, std=d_sqrt_inv)\n",
        "            initializer = tf.random_normal_initializer(stddev=d_sqrt_inv)\n",
        "            return tf.Variable(initial_value=initializer(shape=(n_features, d_token), dtype=\"float32\"),trainable=True,)\n",
        "\n",
        "class NumericalFeatureTokenizer(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_features: int,\n",
        "            d_token: int,\n",
        "            bias: bool,\n",
        "            initialization: str,\n",
        "            ) -> None:\n",
        "        super(NumericalFeatureTokenizer, self).__init__()\n",
        "        \n",
        "        initialization_ = _TokenInitialization.from_str(initialization)\n",
        "        \n",
        "        self.weight = initialization_.apply(n_features, d_token)\n",
        "        self.bias = initialization_.apply(n_features, d_token) if bias else None\n",
        "    \n",
        "    @property\n",
        "    def n_tokens(self) -> int:\n",
        "        \"\"\"The number of tokens.\"\"\"\n",
        "        return self.weight.shape[0]\n",
        "\n",
        "    @property\n",
        "    def d_token(self) -> int:\n",
        "        \"\"\"The size of one token.\"\"\"\n",
        "        return self.weight.shape[1]\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.weight[None] * x[..., None]\n",
        "        if self.bias is not None:\n",
        "            x = x + self.bias[None]\n",
        "        return x\n",
        "\n",
        "from typing import List\n",
        "class CategoricalFeatureTokenizer(tf.keras.layers.Layer):    \n",
        "    def __init__(\n",
        "            self,\n",
        "            cardinalities: List[int],\n",
        "            d_token: int,\n",
        "            bias: bool,\n",
        "            initialization: str,\n",
        "            ) -> None:\n",
        "        super(CategoricalFeatureTokenizer, self).__init__()\n",
        "        \n",
        "        assert cardinalities, 'cardinalities must be non-empty'\n",
        "        assert d_token > 0, 'd_token must be positive'\n",
        "        initialization_ = _TokenInitialization.from_str(initialization)\n",
        "\n",
        "        self.category_offsets = tf.Variable(tf.cast(tf.cumsum([0]+cardinalities[:-1], axis=0),\"float32\"), trainable=False)\n",
        "        #self.embeddings = tf.keras.layers.Embedding(sum(cardinalities), d_token, embeddings_initializer=initialization)\n",
        "        self.embeddings = tf.keras.layers.Embedding(sum(cardinalities), d_token, embeddings_initializer=tf.keras.initializers.Orthogonal)\n",
        "        self.bias = initialization_.apply(len(cardinalities), d_token) if bias else None\n",
        "    \n",
        "    @property\n",
        "    def n_tokens(self) -> int:\n",
        "        \"\"\"The number of tokens.\"\"\"\n",
        "        return self.category_offsets.shape[0]\n",
        "\n",
        "    @property\n",
        "    def d_token(self) -> int:\n",
        "        \"\"\"The size of one token.\"\"\"\n",
        "        return self.embeddings.output_dim\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.embeddings(x + self.category_offsets[None])\n",
        "        if self.bias is not None:\n",
        "            x = x + self.bias[None]\n",
        "        return x"
      ],
      "metadata": {
        "id": "1jGFoEXTlUkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, d_model, num_heads, ff_dim, rate=0.):\n",
        "        super(EncBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(d_model=d_model, num_heads=num_heads, output_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation=\"leaky_relu\"),\n",
        "            tf.keras.layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output, _ = self.att(inputs, inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "VARLEN = X_train.shape[1] # Variable size for each sample\n",
        "EMBED_DIM = 8\n",
        "D_MODEL = 16\n",
        "NUM_HEADS = 1  # Number of attention heads\n",
        "FF_DIM = 16\n",
        "DECAY = 0.999\n",
        "BATCH_SIZE = 2048\n",
        "VIRTUAL_BATCH = 32\n",
        "\n",
        "CARDINALITIES = [2 for _ in range(VARLEN)] # category size for each variable\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(VARLEN))\n",
        "mask = tf.where( tf.math.is_nan(inputs), 0., 1. ) # w/ real value = 1, w/o real value = 0\n",
        "#n_token = tf.expand_dims(BatchNormFCLayer(decay=DECAY)(inputs, mask), -1)\n",
        "#c_token = CategoricalFeatureTokenizer(CARDINALITIES, EMBED_DIM-1, True, 'uniform')(mask)\n",
        "\n",
        "#n_token = BatchNormFCLayer(decay=DECAY)(inputs, mask)\n",
        "n_token = GBN(BATCH_SIZE, VIRTUAL_BATCH, decay=DECAY)(inputs, mask)\n",
        "n_token = NumericalFeatureTokenizer(VARLEN, 4, True, 'uniform')(n_token)\n",
        "c_token = CategoricalFeatureTokenizer(CARDINALITIES, EMBED_DIM-4, True, 'uniform')(mask)\n",
        "x = tf.concat([n_token, c_token], axis=2)\n",
        "\n",
        "x = EncBlock(EMBED_DIM, D_MODEL, NUM_HEADS, FF_DIM)(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(256, activation=\"leaky_relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dense(32, activation=\"leaky_relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dense(4, activation=\"leaky_relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[tf.keras.metrics.AUC(from_logits=True)])\n",
        "\n",
        "import tempfile\n",
        "tempdir = tempfile.TemporaryDirectory()\n",
        "checkpoint_filepath = f\"{tempdir.name}/_test/ckp\"\n",
        "\n",
        "hist = model.fit(X_train,\n",
        "                 y_train,\n",
        "                 validation_data = (X_val,\n",
        "                                    y_val),\n",
        "                 epochs = 30,\n",
        "                 batch_size = BATCH_SIZE,\n",
        "                 shuffle = True,\n",
        "                 callbacks=[\n",
        "                     tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,monitor='val_loss',verbose=0,mode='min',save_best_only=True),\n",
        "                     tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=0.00001),\n",
        "                     tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "                 ] )\n",
        "\n",
        "model.load_weights( tf.train.latest_checkpoint( f\"{tempdir.name}/_test\" ) )\n",
        "tempdir.cleanup()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAhxBGIkkBIy",
        "outputId": "58ec9612-2403-41f9-e9df-e70ba8aa3c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "638/638 [==============================] - 62s 41ms/step - loss: 0.3531 - auc_3: 0.9051 - val_loss: 0.1710 - val_auc_3: 0.9555 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "638/638 [==============================] - 25s 39ms/step - loss: 0.2463 - auc_3: 0.9300 - val_loss: 0.1539 - val_auc_3: 0.9553 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "638/638 [==============================] - 23s 36ms/step - loss: 0.2397 - auc_3: 0.9325 - val_loss: 0.1571 - val_auc_3: 0.9568 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "638/638 [==============================] - 23s 37ms/step - loss: 0.2380 - auc_3: 0.9335 - val_loss: 0.1533 - val_auc_3: 0.9568 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "638/638 [==============================] - 24s 38ms/step - loss: 0.2372 - auc_3: 0.9341 - val_loss: 0.1549 - val_auc_3: 0.9564 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "638/638 [==============================] - 24s 38ms/step - loss: 0.2367 - auc_3: 0.9343 - val_loss: 0.1528 - val_auc_3: 0.9565 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "638/638 [==============================] - 24s 38ms/step - loss: 0.2360 - auc_3: 0.9347 - val_loss: 0.1522 - val_auc_3: 0.9567 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "638/638 [==============================] - 24s 38ms/step - loss: 0.2359 - auc_3: 0.9348 - val_loss: 0.1522 - val_auc_3: 0.9570 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "638/638 [==============================] - 23s 37ms/step - loss: 0.2352 - auc_3: 0.9353 - val_loss: 0.1516 - val_auc_3: 0.9568 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "638/638 [==============================] - 24s 38ms/step - loss: 0.2352 - auc_3: 0.9353 - val_loss: 0.1539 - val_auc_3: 0.9573 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "638/638 [==============================] - 23s 36ms/step - loss: 0.2347 - auc_3: 0.9356 - val_loss: 0.1535 - val_auc_3: 0.9574 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "638/638 [==============================] - 23s 37ms/step - loss: 0.2345 - auc_3: 0.9357 - val_loss: 0.1533 - val_auc_3: 0.9575 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "638/638 [==============================] - 24s 38ms/step - loss: 0.2342 - auc_3: 0.9358 - val_loss: 0.1554 - val_auc_3: 0.9578 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "638/638 [==============================] - 23s 37ms/step - loss: 0.2341 - auc_3: 0.9359 - val_loss: 0.1529 - val_auc_3: 0.9576 - lr: 2.5000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "y_pred = tf.nn.sigmoid(model.predict(X_train, batch_size=BATCH_SIZE))\n",
        "roc_auc = roc_auc_score( y_train, y_pred )\n",
        "print(\"Train\",roc_auc)\n",
        "print(np.mean(y_pred))\n",
        "print(\"\")\n",
        "\n",
        "y_pred = tf.nn.sigmoid(model.predict(X_val, batch_size=BATCH_SIZE))\n",
        "roc_auc = roc_auc_score( y_val, y_pred )\n",
        "print(\"Validation\",roc_auc)\n",
        "print(np.mean(y_pred))\n",
        "print(\"\")\n",
        "\n",
        "y_pred = tf.nn.sigmoid(model.predict(X_test, batch_size=BATCH_SIZE))\n",
        "roc_auc = roc_auc_score( y_test, y_pred )\n",
        "print(\"Testing\",roc_auc)\n",
        "print(np.mean(y_pred))\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "r4AjshjDsgmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455f1f7d-5d2e-4071-daa2-3bf817e85a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "638/638 [==============================] - 21s 22ms/step\n",
            "Train 0.9366207893520464\n",
            "0.17166021\n",
            "\n",
            "116/116 [==============================] - 3s 22ms/step\n",
            "Validation 0.95200657151472\n",
            "0.12402403\n",
            "\n",
            "329/329 [==============================] - 8s 24ms/step\n",
            "Testing 0.9334188055166075\n",
            "0.06553715\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "False_positive_rate, Recall, thresholds = roc_curve( y_test, y_pred )\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(False_positive_rate, Recall, label='(area = %0.4f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kEkn-0TNsXsH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e5f7986d-29f1-48ad-c4a4-15db6c29e186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}