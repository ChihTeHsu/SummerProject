# -*- coding: utf-8 -*-
"""SytheticCurvesSimilarity

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xPdo4eQyoDwVQtYjNRPcb_8b-wvcPFxe
"""

!pip install tensorflow_similarity==0.17.1
!pip install tensorflow_addons
!pip install faiss-cpu

#import tensorflow as tf
import os
#os.environ["PYTHONHASHSEED"] = "777"
os.environ["TF_DETERMINISTIC_OPS"] = "1"
os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
#tf.config.threading.set_intra_op_parallelism_threads(2)
#tf.config.threading.set_inter_op_parallelism_threads(2)

#import random
#import numpy as np
#def set_seed(seed = 777):
#    random.seed(seed)
#    tf.random.set_seed(seed)
#    np.random.seed(seed)

import numpy as np
np.set_printoptions(precision=4)
np.set_printoptions(threshold=np.inf)
np.set_printoptions(linewidth=np.inf)
np.set_printoptions(suppress=True)
np.random.seed(777)

from scipy.special import erf
def IVcurve(x, Vx, Phi, Ti, Of):
    Vsc=7545
    pi=3.1415926
    q=1.6e-19
    k=1.38e-23
    mH=1.67262177774e-27
    mHe=(1.67262177774e-27)*4
    mO=(1.67262177774e-27)*16

    b_H= np.sqrt(mH/(2*k*Ti))
    b_He=np.sqrt(mHe/(2*k*Ti))
    b_O= np.sqrt(mO/(2*k*Ti))

    f_H= Vsc-Vx-np.sqrt( (2*q/mH)* (0.5+0.5*np.tanh(1e+19*(x+Phi))) * (x+Phi) )
    f_He=Vsc-Vx-np.sqrt( (2*q/mHe)*(0.5+0.5*np.tanh(1e+19*(x+Phi))) * (x+Phi) )
    f_O= Vsc-Vx-np.sqrt( (2*q/mO)* (0.5+0.5*np.tanh(1e+19*(x+Phi))) * (x+Phi) )

    IV= lambda b,f :0.5* (1 + erf(b*f) + np.exp(-b*b*f*f)/(np.sqrt(pi)*b*(Vsc-Vx) ))
    return (1-Of)*IV(b_H,f_H)+ Of*IV(b_O,f_O)

RV = np.array(
    [-9.93373125e-04,3.95764436e-01,6.93589837e-01,8.93694317e-01,1.09104005e+00,1.18945533e+00,1.28752994e+00,1.38576693e+00,
     1.58509721e+00,1.78320376e+00,2.17832343e+00,2.47770206e+00,2.87403719e+00,3.17350849e+00,3.57345597e+00,3.87276497e+00,
     4.17414676e+00,4.47308281e+00,4.77574920e+00,4.97875069e+00,5.18096931e+00,5.38201325e+00,5.78615054e+00,6.28558492e+00,
     6.88648143e+00,7.48722301e+00,7.98688117e+00,8.48436093e+00,9.97502566e+00,1.05e+01,1.1e+01,1.2e+01])

ground_pars = []
DATA = []
for i in range(30000):
    Vx = np.random.uniform(-300, 300)
    Phi = -0.1
    Ti = np.random.uniform(1000, 3000)
    Of = np.random.uniform(0, 1)
    ground_pars.append([Vx, Ti, Of])
    DATA.append( IVcurve(RV, Vx, Phi, Ti, Of) )
ground_pars = np.array(ground_pars)
DATA = np.array(DATA)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer

enc = KBinsDiscretizer(n_bins=7, encode="ordinal", strategy='kmeans', random_state=777)
LABELS = enc.fit_transform(ground_pars)

X_train, X_ref, y_train, y_ref = train_test_split(DATA, LABELS, test_size=0.1, random_state=777)
X_ref, X_test, y_ref, y_test = train_test_split(X_ref, y_ref, test_size=0.02, random_state=777)

print(X_train.shape)
print(X_ref.shape)
print(X_test.shape)

import tensorflow as tf
import tensorflow_addons as tfa
import tensorflow_similarity as tfsim
import tensorflow_datasets as tfds

SEED = 26
BATCH_SIZE = 64
INTERP_TO = 64
PROB = 0.3
XGRID = np.linspace(0, 12, INTERP_TO)
#noise = np.random.normal(0, 0.015, X_train.shape)

import typing
def tf_interp(x: typing.Any, xs: typing.Any, ys: typing.Any) -> tf.Tensor:
    ys = tf.convert_to_tensor(ys)
    dtype = ys.dtype

    ys = tf.cast(ys, tf.float64)
    xs = tf.cast(xs, tf.float64)
    x = tf.cast(x, tf.float64)

    xs = tf.concat([[xs.dtype.min], xs, [xs.dtype.max]], axis=0)
    ys = tf.concat([ys[:1], ys, ys[-1:]], axis=0)

    ms = (ys[1:] - ys[:-1]) / (xs[1:] - xs[:-1])
    ms = tf.pad(ms[:-1], [(1, 1)])

    bs = ys - ms*xs

    i = tf.math.argmax(xs[..., tf.newaxis, :] > x[..., tf.newaxis], axis=-1)
    m = tf.gather(ms, i, axis=-1)
    b = tf.gather(bs, i, axis=-1)

    y = m*x + b
    return tf.cast(tf.reshape(y, tf.shape(x)), dtype)

# https://stackoverflow.com/questions/58546373/how-to-add-randomness-in-each-iteration-of-tensorflow-dataset
# https://stackoverflow.com/questions/69108284/tf-data-dataset-map-functionality-and-random
#def custom_augment(ri):
#    return tf.nn.dropout(tf_interp(XGRID, RV, ri + tf.random.normal((ri.shape[-1],), 0, 0.015, dtype=ri.dtype)), rate=PROB)*(1-PROB)
RNG = tf.random.Generator.from_seed(33)
def custom_augment(ri):
    return tf.nn.experimental.general_dropout(tf_interp(XGRID, RV, ri + RNG.normal((ri.shape[-1],), 0, 0.015, dtype=ri.dtype)), PROB, RNG.uniform)*(1-PROB)

train_one = (
    tf.data.Dataset.from_tensor_slices( X_train ).shuffle(buffer_size=4096, seed=SEED)
    .map(custom_augment, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)
train_two = (
    tf.data.Dataset.from_tensor_slices( X_train ).shuffle(buffer_size=4096, seed=SEED)
    .map(custom_augment, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)
train_ds = tf.data.Dataset.zip((train_one, train_two))

ref_ds = tf.data.Dataset.from_tensor_slices( X_ref ).map(lambda ri: tf_interp(XGRID, RV, ri), num_parallel_calls=tf.data.AUTOTUNE).batch(9)
test_ds = tf.data.Dataset.from_tensor_slices( X_test ).map(lambda ri: tf_interp(XGRID, RV, ri), num_parallel_calls=tf.data.AUTOTUNE).batch(9)

import matplotlib.pyplot as plt

sample_one, sample_two = next(iter(train_ds))

plt.figure()
for n in range(9):
    ax = plt.subplot(3, 3, n+1)
    plt.plot(XGRID, sample_one[n],'-', label="one")
    plt.plot(XGRID, sample_two[n],'--',label="two")
    plt.axis("off")
plt.show()

import faiss
from scipy import stats
from tensorflow_similarity.callbacks import EvalCallback
from tensorflow_similarity.utils import unpack_results
from collections.abc import MutableMapping

class myCallback(EvalCallback):
    def __init__(self, queries, query_labels, targets, target_labels, k):
        super().__init__(queries, query_labels, targets, target_labels)
        """
        Args:
            queries: Test examples that will be tested against the built index.
            query_labels: Test examples expected ground truth labels.
            targets: Reference examples that are used to build index.
            target_labels: Reference examples labels.
            k: Number of neighbors to return for each query.
        """
        self.target_labels = target_labels # initial as numpy for fancy indexing later
        self.k = k

    def on_epoch_end(self, epoch: int, logs: MutableMapping | None = None):
        _ = epoch
        if logs is None:
            logs = {}

        # get the embeddings
        reference = self.model.backbone.predict(self.targets, verbose=0)
        test = self.model.backbone.predict(self.queries_known, verbose=0)

        # rebuild the index from reference
        index = faiss.IndexFlatL2(reference.shape[1])
        index.add(reference)

        # predict the test
        distances, indices = index.search(test, k=self.k)

        # lookup reference label via predicted indices
        votes = self.target_labels[indices] # unless self.target_labels is initial as numpy, otherwise we cannot use fancy indexing here
        #votes = tf.stack([self.target_labels.numpy()[indices[:,k]] for k in range(self.k)], axis=1) # cannot directly use fancy indexing style in the tensorflow

        # take mode as prediction output
        Vxmode, _ = stats.mode(votes[:,:,0],axis=-1)
        Timode, _ = stats.mode(votes[:,:,1],axis=-1)
        Ofmode, _ = stats.mode(votes[:,:,2],axis=-1)

        # count how many times it correct
        VxRes = np.sum(self.query_labels_known[:, 0] == Vxmode)
        TiRes = np.sum(self.query_labels_known[:, 1] == Timode)
        OfRes = np.sum(self.query_labels_known[:, 2] == Ofmode)

        known_results = {"Vx": VxRes}, {"Ti": TiRes}, {"Of": OfRes}
        for a in known_results:
            unpack_results(
                a,
                epoch=epoch,
                logs=logs,
                tb_writer=self.tb_writer,
            )
        self.model.reset_index()

TEMPERATURE = 0.25
INIT_LR = 1e-3
EPOCHS = 50
HID_SIZE = 16

def get_backbone(input_dim, channel=16):
    inputs = tf.keras.layers.Input((input_dim,), name="backbone_input")
    x = tf.expand_dims(inputs, axis=-1)
    x = tf.keras.layers.Conv1D(channel, kernel_size=3, strides=2, activation="relu")(x)
    x = tf.keras.layers.Conv1D(channel, kernel_size=3, strides=2, activation="relu")(x)
    x = tf.keras.layers.Flatten()(x)
    outputs = tf.keras.layers.Dense(channel, activation="relu", name="backbone_output")(x)
    return tf.keras.Model(inputs, outputs, name="backbone")

backbone = get_backbone( INTERP_TO, channel=32)
projector = tfsim.models.contrastive_model.get_projector( input_dim = backbone.output.shape[-1], dim=HID_SIZE, num_layers=2)

model = tfsim.models.ContrastiveModel(
    backbone = backbone,
    projector = projector,
    algorithm="simclr"
)

model.compile(
    optimizer = tfa.optimizers.LAMB(learning_rate=INIT_LR),
    loss = tfsim.losses.SimCLRLoss(name="simclr", temperature=TEMPERATURE)
)

history = model.fit(
    train_ds,
    epochs = EPOCHS,
    callbacks=[
        myCallback(test_ds, y_test, ref_ds, y_ref, k=4)
    ],
    verbose = 0,
)

import matplotlib.pyplot as plt
plt.figure(figsize=(17,4))

ax1 = plt.subplot(141)
plt.plot(history.history["loss"])
plt.grid()
plt.title("simclr_loss")

ax2 = plt.subplot(142)
plt.plot(history.history["Vx"], label="error")
plt.grid()
plt.title("Vx")

ax3 = plt.subplot(143)
plt.plot(history.history["Ti"], label="error")
plt.grid()
plt.title("Ti")

ax4 = plt.subplot(144)
plt.plot(history.history["Of"], label="error")
plt.grid()
plt.title("Of")

plt.show()